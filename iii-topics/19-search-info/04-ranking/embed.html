




	
	
		

	
	
		

	
	
		

	
	
		
		

	
	
		

	
	
		
		

	
	
		
		
		
		

	
	
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		

	
	
		

	
	
		

	
	
		
		
		

	
	
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		

	
	
		

	
	
		

	
	
		

	
	
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		
		
		

	
	
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		

	
	
		

	
	
		
		
<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.117.0">
    <meta name="generator" content="Relearn 5.18.0">
    <meta name="description" content="Intro CS Textbook">
    <meta name="author" content="K-State CS Faculty">
    <title>Page Rank :: Intro CS Textbook</title>
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/iii-topics/19-search-info/04-ranking/index.html" rel="canonical" type="text/html" title="Page Rank :: Intro CS Textbook">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/iii-topics/19-search-info/04-ranking/index.xml" rel="alternate" type="application/rss+xml" title="Page Rank :: Intro CS Textbook">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/iii-topics/19-search-info/04-ranking/index.print.html" rel="alternate" type="text/html" title="Page Rank :: Intro CS Textbook">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/iii-topics/19-search-info/04-ranking/tele.html" rel="alternate" type="text/html" title="Page Rank :: Intro CS Textbook">
    
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/fontawesome-all.min.css?1701489569" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://ksu-cs-textbooks.github.io/cs-zero/css/fontawesome-all.min.css?1701489569" rel="stylesheet"></noscript>
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/nucleus.css?1701489569" rel="stylesheet">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/auto-complete.css?1701489569" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://ksu-cs-textbooks.github.io/cs-zero/css/auto-complete.css?1701489569" rel="stylesheet"></noscript>
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/perfect-scrollbar.min.css?1701489569" rel="stylesheet">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/fonts.css?1701489569" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://ksu-cs-textbooks.github.io/cs-zero/css/fonts.css?1701489569" rel="stylesheet"></noscript>
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/theme.css?1701489569" rel="stylesheet">
    
    
    
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/theme-light-theme.css?1701489569" rel="stylesheet" id="variant-style">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/variant.css?1701489569" rel="stylesheet">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/print.css?1701489569" rel="stylesheet" media="print">
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/ie.css?1701489569" rel="stylesheet">
    <script src="https://ksu-cs-textbooks.github.io/cs-zero/js/url.js?1701489569"></script>
    
    
    
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="https://ksu-cs-textbooks.github.io/cs-zero/index.search.js";
      var root_url="https://ksu-cs-textbooks.github.io/cs-zero/";
      var baseUri=root_url.replace(/\/$/, '');
      // translations
      window.T_Copy_to_clipboard = 'Copy to clipboard';
      window.T_Copied_to_clipboard = 'Copied to clipboard!';
      window.T_Copy_link_to_clipboard = 'Copy link to clipboard';
      window.T_Link_copied_to_clipboard = 'Copied link to clipboard!';
      window.T_No_results_found = 'No results found for \u0022{0}\u0022';
      window.T_N_results_found = '{1} results found for \u0022{0}\u0022';
      // some further base stuff
      var baseUriFull='https:\/\/ksu-cs-textbooks.github.io\/cs-zero/';
      window.variants && variants.init( [ 'light-theme', 'dark-theme' ] );
    </script>
    
    <link href="https://ksu-cs-textbooks.github.io/cs-zero/css/custom.css?1701489569" rel="stylesheet">

    
  </head>
  <body class="mobile-support embed disableInlineCopyToClipboard" data-url="https://ksu-cs-textbooks.github.io/cs-zero/iii-topics/19-search-info/04-ranking/embed.html">
    <div id="body" class="default-animation">
      
      
      
      <main id="body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
    
    
    

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/7TCUh_fkwrY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h4 id="resources">Resources</h4>
<ul>
<li><a href="../slides/19-InfoRetrieval.pdf">Slides</a>
</li>
</ul>
<h4 id="video-script">Video Script</h4>
<p>Welcome back. In this video, we&rsquo;re going to be taking a deeper look into what kind of queries we can actually ask of Google or searching the web. So we&rsquo;ve already looked at simple queries like cat, dog, or cat and dog, or cat or dog. But that&rsquo;s really not as interesting of a question to ask of the internet, right? When we&rsquo;re wanting to find all of the webpages that are about cats, not just have the occurrences of the word cat, but are about cats. And so how do you know which web pages are more likely to be about cats and webpages that are more likely to be about dogs? And how could you tell the difference? And how would you modify our search algorithm that we had talked about earlier to account for that. So this ranking of the internet becomes a little bit more complicated than our simple straight up text queries.</p>
<p>But as we learned in the HTML lectures, pages in the world wide web have structure to them in the form of HTML tags. And so we could add more information about those tags called metawords to our indexing algorithm. And so now we could could re query the index to find specific words in. for example, the title. So in theory, right at a very simple explanation is a article that contained the word cat in the title is probably more likely to be about cats than it is dogs, even though dog was found on that webpage somewhere in the text. But again, this is a very simple approach, right? The metawords help our algorithm, help our searching, but it&rsquo;s not going to be a pure ranking system, right? Because basing our questions and answers off of just the text, and these tags won&rsquo;t yield very efficient results. And it won&rsquo;t yield a very good collection or a very good curated, top 10 list of dogs? And how do we rank them in priority from our search? Because obviously, there&rsquo;s a lot of cats and dogs on the internet. So how does one webpage about cats come first, early web search engines dealt with. So as the web got larger and larger, it was more and more difficult to find good pages in all of the bad or useless ones.</p>
<p>One of the early search engines was actually called AltaVista, it grew very quickly at first. So in 1996, it had five servers, 210 gigs of search engine that was constantly indexing the web, late 90s. So having 500 gigs of storage, and 130 gigs of RAM was big, right? This was a huge, huge set of servers, and very powerful machine in order to index web. And it was able to handle 13 million queries daily. So considering this the late 90s, Internet was still relatively new, but this was also getting very close to the .com era. But as we mature  into the .com era, Alta Vista was eclipsed by another. So Yahoo, came in and bought them out in 2003, and then eventually shut them down in 2011. But Alta Vista was one of the first major successful search engines that we had for the internet.</p>
<p>We&rsquo;ll transition into technology that was developed and published in a paper called the Anatomy of a Large-Scale Hypertextual Web Search Engine that provided the answer to our question of how do we rank the internet. They created a new algorithm called PageRank, named after Page himself, that could rank search engine results based off of their authority of the page that was actually found  But let&rsquo;s see how this actually works.</p>
<p>So, as you know, the worldwide web is interconnected ith hyperlinks. So if we had two hypothetical web pages here, Ernie&rsquo;s scrambled egg recipe and Bert&rsquo;s scrambled egg recipe. And we could analyze those links that are going o and from that page to learn more about it. With our previous indexing algorithm, if we found eggs on both of these pages, so they&rsquo;re technically about eggs, but which one should come first. Should Ernie&rsquo;s come first in our search results, or Bert&rsquo;s? Which one of these do you think would be the top result for a scrambled egg recipe and why? So in theory, right, Ernie&rsquo;s scrambled recipe, even if it is better than Bert&rsquo;s, Ernie s recipe only has one other web page that links to it. So this could be a comment, it could be another blogger, whatever it may be, maybe a Facebook post. But there&rsquo;s only one person that has a link to that webpage. But Bert&rsquo;s recipe is obviously more popular, or at least more well known, because it has three pages that link to it. So according to the page rank, here, Bert&rsquo;s recipe has more authority about scrambled eggs, then Ernie as on scrambled eggs. So Bert&rsquo;s would come first in our search result and Ernie&rsquo;s would come next, if these were only two web pages about scrambled eggs. But unfortunately, the World Wide Web is not as nicely structured as we like. So it is highly dependent on the actual websites themselves and other things that become a lot more difficult to actually search through.</p>
<p>But what about this particular example, which page would be ranked higher? Well, we still have Ernie&rsquo;s and Bert&rsquo;s scrambled egg recipe, we have John and Alice here. John&rsquo;s web page points to Ernie&rsquo;s and Alice&rsquo;s points to Bert&rsquo;s. Ernie&rsquo;s recipe only tried it once it&rsquo;s not too bad. Alice is clearly a lot more excited about Bert&rsquo;s recipe. Bert&rsquo;s recipe is clearly one of the best recipes. So if we are just basing this off of the text, Alice has more authority or would have more authority than John, if we&rsquo;re looking at the sentiment or the excitement here. But this becomes a problem if we&rsquo;re just looking at links, though. Right? One link here, one link here. Which one comes first? How do we break that tie? Well, let&rsquo;s take a look at it as far as the authority of the pages that actually link to it. So John and Alice both have a link to Ernie and Bert&rsquo;s scrambled egg recipe.</p>
<p>So that&rsquo;s one and one for both Ernie and Bert. But John is a new blogger; John doesn&rsquo;t have very many followers so John does not have very much authority. So John has only two links pointing to his web page. But Alice is a really popular blogger. And so she has a lot of different pages, maybe she has 100 different pages that are pointing to that page. And so Alice ends up having more authority. And since Alice has more authority, a link from Alice to Bert&rsquo;s scrambled egg recipe has far more weight than John linking to Ernie&rsquo;s scrambled egg recipe. And so in that sense, Bert&rsquo;s recipe would still come first in our page ranking algorithm over Ernie&rsquo;s scrambled egg recipe because Alice has more weight, more importance, more authority than John&rsquo;s page, even though Ernie and Bert have the same number of links to their websites.</p>
<p>But sadly, again, right the internet is not a nice tree structure like that either. Sometimes it&rsquo;s full of these cyclic connections like this. So if we follow for example, A to B to E to A to B, E A to B to E, we get stuck in these cycles. And these cycles are what we call syncs. Okay, so they suck up all of the authority of the internet. Because our algorithm gets stuck  So if our indexer is just sitting there out there on the eb following links, it&rsquo;ll just et stuck in this cycle. It&rsquo;ll follow that link over that cycle over and over and over and ver and over and over again. And so all of the authority gets consumed by that cycle. So we can&rsquo;t just simply follow links on the web because we&rsquo;ll get stuck in those an all other web pages will have no authority whatsoever.</p>
<p>So one of the ways that actually Page and Brin suggested is called this random surfer model. So imagining that you are simulating the random surfer, you&rsquo;re going to start on some web page, totally at random on the internet, and then just start clicking on random links. And you&rsquo;ll continue on that pattern until at some random point in the future, you teleport to a new page and start surfing again. So start on a page, start clicking links on that web page, and then, and then you&rsquo;ll eventually get ported off into another web page. And then you&rsquo;ll start this process over again. And so instead of just selecting each link, in a specific order on a web page, you&rsquo;re going to randomly select links to actually follow, this will cause you to break out of those cycles. And if you do this enough time, keeping track of the percentage of visits that you made onto a particular web page, you can use those values as the rank of the page. So the randomness here, the more links that you have to a page, theoretically speaking, you are more likely to be randomly surfed to others.</p>
<p>So let&rsquo;s take a look at a  example of the random surfer w th PageRank. So here is our sample web that we&rsquo;re actually crawling here. So we have site A goes to home has an about page, product page, and more on the product page, links to another we site. So site A has these four pages, and then the product page on site a links to site B. Now, let&rsquo;s say we are just rolling a set of dice, and each of these web pages has a link and that die, the probability the number on that die will would have you either follow that link, or stay on that webpage, right. If multiple links on were, the number that was rolled on the die, depends on which link you actually follow. If you keep on randomly rolling the die, and following links here, which page actually has more authority in this particular example? So if we kept on doing our random page search, so let&rsquo;s see, if we started at S te A, and we went to home, we could have, we have three different options here. So three outgoing links, and three incoming links. And same thing for about has one, two, three outgoing and three incoming, and so does the more web page. So it has three outgoing and three incoming.</p>
<p>A couple differences here: home has four incoming links, and product has four outgoing links. And so if we keep on randomly rolling our dice here, this is what the authority would actually look like. So if we went long enough, the about and more page would be identical in authority because due to probability, they&rsquo;re equally as probable to be visited as the other because they bo h have three outgoing links, and three incoming links. The product page has a little bit more authority than the about and more page because there are three or four outgoing links, and only three incoming links. So they have a few extra inks on here that it can actually follow even though it causes the product page to go to site B. Site B only has one possible way of actually being visited, and so that has the smallest authority. But the homepage has the highest authority among all of the pages on site a because it has four incoming links and three outgoing links. So probability wise, it is more likely to be visited than all of the other pages here. And that&rsquo;s the idea of the random surfer model, even though we have a cycle here. So from home, the more the product about home more product about. Even though we have cycles, the random surfer model is going to narrow down that cycle regardless of the cycle and narrow down which web page is actually more likely to be visited based off of the links that actually finds.</p>
<p>Another way you can do this is by simply generating all possible paths that can be found on the internet. And you can calculate the length of those paths and the percentage of the times that those particular pages show up in those paths. So if we calculate all the different paths that we can actually find here that are less than five, so five sites deep, we actually find that site a occurs more frequently in those past than all others. So this is a little bit different than the random surfer model. So this is a little bit more discreet, meaning that we have a little bit more of an order to things instead of everything being completely random. It does help solve the random surfer model, although we do have to limit the length of our path. So there is some downsides to this technique.</p>
<p>But of course, there&rsquo;s a lot of different problems out there that could get in our way of actually searching data, right? Instead of just those cycles, right? We have a lot of other issues like spam. How do we take into account our all those ads that are on the internet that are just basically plugging the internet, although that&rsquo;s, you know, how a lot of people make their money online. But how do you avoid spam? How do you avoid ads that you don&rsquo;t want to search? How do you avoid malicious websites? You don&rsquo;t want a malicious website to be the number one hit on Google. How do you take into account the size of the internet and reduce computation time? How do you search non textual information? Right? We have a lot of images and videos on the internet now. And audio, how do you take that into account? Structured versus unstructured data and a lot of lots of different other things. And that brings us really into the difference between what we had with web 1.0 to web 2.0. This has a such a major impact on how the world wide web works. And so after the .com bubble burst, no one really knew where the internet was headed. And a lot of different talks were being had about what the internet could be, or what it could be different, right?</p>
<p>Because at the time, the internet was a place where companies just push content to their users. But now the internet is a place where users can communicate and share their own content quickly and easily. And eventually, that led to this idea of web 2.0, which we&rsquo;ve talked about before. But the reason that we bring this back up is that the web is a dynamic place now or the web is a more complicated place now. We have web 1.0. We had static web pages that didn&rsquo;t change very often. And so that led to search algorithms that were significant easier to do because we were dependent mostly on text. And now we have dynamic web pages where the web page that I visit is entirely different than the webpage you visit even if it&rsquo;s the same URL, because it depends on who&rsquo;s logged in. It depends on your browsing history. Now we&rsquo;re also looking at user generated content. So if you look at a Reddit post, right, Reddit trying to search Reddit, how does that show up in Google? Because new Reddit posts are made every single second. So how do you take that into account? How do you take into account multimedia pages, so things like YouTube videos, audio, songs, images, pictures of cats versus pictures of dogs when you&rsquo;re trying to search that versus text. And this is an entirely different web that we live in now versus what we had in the 90s when early search engines were about. And so this is something that is going to be a continuous problem as we kind of move forward and as the web grows, but it&rsquo;s just kind of highlighting the importance of this sophistication of the search engines that we are currently able to use.</p>

            <footer class="footline">

            </footer>
          </article>
        </div>
      </main>
    </div>
    
    
    
    <script src="https://ksu-cs-textbooks.github.io/cs-zero/js/clipboard.min.js?1701489573" defer></script>
    <script src="https://ksu-cs-textbooks.github.io/cs-zero/js/perfect-scrollbar.min.js?1701489573" defer></script>
    <script src="https://ksu-cs-textbooks.github.io/cs-zero/js/theme.js?1701489573" defer></script>
    
    <script src="https://ksu-cs-textbooks.github.io/cs-zero/js/embed-iframe.js?1701489573 defer"></script>
  </body>
</html>
